
---

# ğŸ“š Document Intelligence RAG System

## ğŸ§  Overview

This project is an advanced **Document Intelligence System** built using **Retrieval-Augmented Generation (RAG)**.
It enables users to upload documents (PDF/TXT/DOCX), interact with them using a chatbot, highlight keywords in PDFs, and evaluate RAG performance through automated experiments.

The system integrates:

* âš¡ Hybrid RAG (Extractive + Generative Models)
* ğŸ§© Adaptive Chunking
* ğŸ” Semantic Retrieval using Embeddings
* ğŸ¤– Question Answering (DistilBERT + Phi-3)
* ğŸŒ Streamlit UI
* ğŸ“Š Research & Evaluation Framework
* ğŸŒŠ Neptune.ai Experiment Tracking
* ğŸ“ˆ Excel-based Experiment Reports

This project is designed both as a **production-ready RAG application** and a **research framework** for evaluating RAG systems.

---

## ğŸš€ Features

### 1ï¸âƒ£ Document Chat (RAG-based)

* Upload PDF / TXT / DOCX files
* Ask questions about the document
* Hybrid QA system:

  * Extractive model: DistilBERT
  * Generative model: Phi-3
* Context-aware answers using retrieved chunks

---

### 2ï¸âƒ£ Adaptive Chunking

* Automatically determines chunk size based on document length
* Prevents chunk explosion using hard limits
* Optimized for:

  * Small documents
  * Medium documents
  * Large documents

---

### 3ï¸âƒ£ Semantic Retrieval

* Sentence Transformers (`all-MiniLM-L6-v2`)
* Cosine similarity-based chunk retrieval
* Top-k relevant chunk selection

---

### 4ï¸âƒ£ PDF Keyword Highlighting

* Highlight user-defined keywords in PDFs
* Shows:

  * Found keywords
  * Missing keywords
* Generates highlighted PDF output

---

### 5ï¸âƒ£ Streamlit User Interface

* Document upload panel
* Chatbot interface
* PDF viewer with highlights
* Keyword highlighting panel

---

### 6ï¸âƒ£ Research & Evaluation Framework

Located in the `research/` folder.

Supports:

* Multiple documents testing
* Multiple prompts per document
* Multiple questions per document
* Automated RAG evaluation
* Model comparison
* Chunking experiments
* Prompt experiments

---

### 7ï¸âƒ£ Neptune.ai Integration

* Logs experiments in real-time
* Tracks:

  * Questions
  * Answers
  * Retrieved chunks
  * Response time
  * Model used
  * Document metadata
* Visual dashboards for RAG analysis

---

### 8ï¸âƒ£ Excel Experiment Report

* Automatically generated after experiments
* Contains:

  * Document name
  * Prompt
  * Question
  * Answer
  * Chunks used
  * Chunk size
  * Response time
  * Model used

---

## ğŸ—ï¸ Project Architecture

```
document_intelligence/
â”‚
â”œâ”€â”€ app.py                     # Streamlit UI
â”œâ”€â”€ config.py                  # Configuration settings
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ src/                       # Core RAG pipeline
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ qa_model.py
â”‚   â”œâ”€â”€ adaptive_chunker.py
â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”œâ”€â”€ pdf_highlighter.py
â”‚
â”œâ”€â”€ research/                  # RAG evaluation framework
â”‚   â”œâ”€â”€ test_cases.py
â”‚   â”œâ”€â”€ experiment_runner.py
â”‚   â”œâ”€â”€ neptune_monitor.py
â”‚   â”œâ”€â”€ model_comparison.py
â”‚   â”œâ”€â”€ chunk_experiments.py
â”‚   â”œâ”€â”€ prompt_experiments.py
â”‚   â”œâ”€â”€ report_generator.py
â”‚
â”œâ”€â”€ data/                      # Uploaded documents
â”œâ”€â”€ highlighted_pdfs/          # Output PDFs
â”œâ”€â”€ temp_images/
â””â”€â”€ rag_test_results.xlsx      # Experiment results
```

---

## ğŸ§  Hybrid RAG Pipeline Flow

### ğŸŸ¢ Application Mode (Streamlit)

```
Document Upload
      â†“
Text Extraction
      â†“
Adaptive Chunking
      â†“
Embeddings Generation
      â†“
Semantic Retrieval
      â†“
Hybrid QA Models
(DistilBERT + Phi-3)
      â†“
Answer Generation
      â†“
PDF Highlighting + Chat UI
```

---

### ğŸ”µ Research Mode (Experiment Runner)

```
Multiple Documents
      â†“
Automated Test Cases
      â†“
Adaptive Chunking + Retrieval
      â†“
Hybrid QA Models
      â†“
Neptune Logging
      â†“
Excel Report Generation
```

---

## ğŸ¤– Models Used

### ğŸ”¹ Embedding Model

* `sentence-transformers/all-MiniLM-L6-v2`

### ğŸ”¹ Extractive QA Model

* `distilbert-base-cased-distilled-squad`

### ğŸ”¹ Generative LLM

* `microsoft/phi-3-mini-4k-instruct`

---

## ğŸ› ï¸ Technologies & Platforms

| Category            | Tools                    |
| ------------------- | ------------------------ |
| Language            | Python                   |
| UI                  | Streamlit                |
| NLP Models          | HuggingFace Transformers |
| Embeddings          | Sentence Transformers    |
| PDF Processing      | PyMuPDF (fitz)           |
| Experiment Tracking | Neptune.ai               |
| Data Analysis       | Pandas, Excel            |
| ML Ops              | Hybrid RAG Pipeline      |
| GPU Support         | PyTorch + CUDA           |

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone <repo-url>
cd document_intelligence
```

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
.\.venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Run the Streamlit App

```bash
streamlit run app.py
```

Open in browser:

```
http://localhost:8501
```

---

## ğŸ§ª Run RAG Experiments

```bash
python research/experiment_runner.py
```

---

## ğŸŒŠ Enable Neptune.ai

Set environment variables:

```powershell
setx NEPTUNE_PROJECT "your-workspace/your-project"
setx NEPTUNE_API_TOKEN "your-api-token"
```

Restart terminal and run experiments again.

---

## ğŸ“Š Output Files

* `rag_test_results.xlsx` â†’ Experiment results
* Neptune Dashboard â†’ Visual RAG analysis
* Highlighted PDFs â†’ Keyword visualization

---

## ğŸ”¬ Research Capabilities

This system supports:

* Multi-document RAG testing
* Prompt engineering experiments
* Chunk size analysis
* Model comparison (Extractive vs Generative)
* Performance benchmarking
* RAG reliability testing
* Negative question testing (hallucination detection)

---

## ğŸ’¡ Key Insights from the Project

* Hybrid RAG improves accuracy and speed
* Adaptive chunking must be constrained
* Extractive models are faster for factual queries
* Generative models are better for reasoning
* Experiment tracking is essential for RAG evaluation

---

## ğŸ¯ Use Cases

* Document Q&A Systems
* Resume Analysis
* Academic Document Understanding
* Legal / Policy Document Search
* Research Paper Analysis
* Enterprise Knowledge Assistants
* RAG Benchmarking Framework

---

## ğŸ§‘â€ğŸ’» Author

**Ruffina Mercy S**
Aspiring AI Engineer & Data Scientist
Project: Document Intelligence RAG System

---

